{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.metrics import f1_score\n",
    "from pandas import read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_csv('data/olid-training-v1.0.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet subtask_a  \\\n",
       "0  86426  @USER She should ask a few native Americans wh...       OFF   \n",
       "1  90194  @USER @USER Go home you’re drunk!!! @USER #MAG...       OFF   \n",
       "2  16820  Amazon is investigating Chinese employees who ...       NOT   \n",
       "3  62688  @USER Someone should'veTaken\" this piece of sh...       OFF   \n",
       "4  43605  @USER @USER Obama wanted liberals &amp; illega...       NOT   \n",
       "\n",
       "  subtask_b subtask_c  \n",
       "0       UNT       NaN  \n",
       "1       TIN       IND  \n",
       "2       NaN       NaN  \n",
       "3       UNT       NaN  \n",
       "4       NaN       NaN  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "labels = {\n",
    "    'a': [],\n",
    "    'b': [],\n",
    "    'c': []\n",
    "}\n",
    "for index, row in df.iterrows():\n",
    "    texts.append(row['tweet'])\n",
    "    labels['a'].append(row['subtask_a'])\n",
    "    labels['b'].append(row['subtask_b'])\n",
    "    labels['c'].append(row['subtask_c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "label_index = {\n",
    "    'NOT': 0,\n",
    "    'OFF': 1,\n",
    "    'UNT': 0,\n",
    "    'TIN': 1,\n",
    "    'IND': 0,\n",
    "    'GRP': 1,\n",
    "    'OTH': 2\n",
    "}\n",
    "\n",
    "label_index = defaultdict(bool, label_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map label from str to int\n",
    "for subtask in labels:\n",
    "    labels[subtask] = list(map(lambda x: label_index[x], labels[subtask]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer():\n",
    "    def __init__(self, num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ', oov_token=None):\n",
    "        self.tokenizer = TweetTokenizer()\n",
    "        self.word_counts = OrderedDict()\n",
    "        self.word_index = dict()\n",
    "        self.index_word = dict()\n",
    "        self.word_docs = defaultdict(int)\n",
    "        self.num_words = num_words\n",
    "        self.filters = filters\n",
    "        self.lower = lower\n",
    "        self.split = split\n",
    "        self.oov_token = oov_token\n",
    "\n",
    "    def fit_on_texts(self, texts):\n",
    "        for text in texts:\n",
    "            seq = self.text_to_word_sequence(text, self.filters, self.lower, self.split)\n",
    "            for w in seq:\n",
    "                if w in self.word_counts:\n",
    "                    self.word_counts[w] += 1\n",
    "                else:\n",
    "                    self.word_counts[w] = 1\n",
    "            for w in set(seq):\n",
    "                # In how many documents each word occurs\n",
    "                self.word_docs[w] += 1\n",
    "\n",
    "        wcounts = list(self.word_counts.items())\n",
    "        wcounts.sort(key=lambda x: x[1], reverse=True)\n",
    "        # forcing the oov_token to index 1 if it exists\n",
    "        if self.oov_token is None:\n",
    "            sorted_voc = []\n",
    "        else:\n",
    "            sorted_voc = [self.oov_token]\n",
    "        sorted_voc.extend(wc[0] for wc in wcounts)\n",
    "\n",
    "        # note that index 0 is reserved, never assigned to an existing word\n",
    "        self.word_index = dict(\n",
    "            list(zip(sorted_voc, list(range(1, len(sorted_voc) + 1)))))\n",
    "\n",
    "        self.index_word = dict((c, w) for w, c in self.word_index.items())\n",
    "        \n",
    "    def text_to_word_sequence(self, text,\n",
    "                              filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                              lower=True, split=' '):\n",
    "        \"\"\"Converts a text to a sequence of words (or tokens).\n",
    "\n",
    "        # Arguments\n",
    "            text: Input text (string).\n",
    "            filters: list (or concatenation) of characters to filter out, such as\n",
    "                punctuation. Default: ``!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n``,\n",
    "                includes basic punctuation, tabs, and newlines.\n",
    "            lower: boolean. Whether to convert the input to lowercase.\n",
    "            split: str. Separator for word splitting.\n",
    "\n",
    "        # Returns\n",
    "            A list of words (or tokens).\n",
    "        \"\"\"\n",
    "        text = re.sub(r'@USER|URL', split, text)\n",
    "        \n",
    "        if lower:\n",
    "            text = text.lower()\n",
    "\n",
    "        text = re.sub(r'bi\\*ch|b\\*\\*ch|bi\\*\\*h|biatch', 'bitch', text)\n",
    "        text = re.sub(r'sob|sobi*ch', 'son of bitch', text)\n",
    "        text = re.sub(r'f\\*\\*k|f\\*ck|fu\\*k', 'fuck', text)\n",
    "        text = re.sub(r'[\\'’]s', ' is', text)\n",
    "        text = re.sub(r'[\\'’]re', ' are', text)\n",
    "\n",
    "        translate_dict = dict((c, split) for c in filters)\n",
    "        translate_map = str.maketrans(translate_dict)\n",
    "        text = text.translate(translate_map)\n",
    "        \n",
    "        seq = self.tokenizer.tokenize(text)\n",
    "\n",
    "        return [i for i in seq if i]\n",
    "    \n",
    "    def texts_to_sequences_generator(self, texts):\n",
    "        \"\"\"Transforms each text in `texts` to a sequence of integers.\n",
    "\n",
    "        Each item in texts can also be a list,\n",
    "        in which case we assume each item of that list to be a token.\n",
    "\n",
    "        Only top \"num_words\" most frequent words will be taken into account.\n",
    "        Only words known by the tokenizer will be taken into account.\n",
    "\n",
    "        # Arguments\n",
    "            texts: A list of texts (strings).\n",
    "\n",
    "        # Yields\n",
    "            Yields individual sequences.\n",
    "        \"\"\"\n",
    "        num_words = self.num_words\n",
    "        oov_token_index = self.word_index.get(self.oov_token)\n",
    "        for text in texts:\n",
    "            seq = self.text_to_word_sequence(text,\n",
    "                                             self.filters,\n",
    "                                             self.lower,\n",
    "                                             self.split)\n",
    "            vect = []\n",
    "            for w in seq:\n",
    "                i = self.word_index.get(w)\n",
    "                if i is not None:\n",
    "                    if num_words and i >= num_words:\n",
    "                        if oov_token_index is not None:\n",
    "                            vect.append(oov_token_index)\n",
    "                    else:\n",
    "                        vect.append(i)\n",
    "                elif self.oov_token is not None:\n",
    "                    vect.append(oov_token_index)\n",
    "            yield vect\n",
    "            \n",
    "    def texts_to_sequences(self, texts):\n",
    "        \"\"\"Transforms each text in texts to a sequence of integers.\n",
    "\n",
    "        Only top \"num_words\" most frequent words will be taken into account.\n",
    "        Only words known by the tokenizer will be taken into account.\n",
    "\n",
    "        # Arguments\n",
    "            texts: A list of texts (strings).\n",
    "\n",
    "        # Returns\n",
    "            A list of sequences.\n",
    "        \"\"\"\n",
    "        return list(self.texts_to_sequences_generator(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-task a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.layers import LSTM, RNN, GRU, Bidirectional\n",
    "from keras.layers import Concatenate, Flatten, Reshape, Dropout\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.twitter.27B')\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "MAX_NUM_WORDS = 30000\n",
    "EMBEDDING_DIM = 200\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.twitter.27B.200d.txt')) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = CustomTokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19514 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (13240, 200)\n",
      "Shape of label tensor: (13240, 2)\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels_a = to_categorical(np.asarray(labels['a']))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels_a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels_a = labels_a[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels_a[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels_a[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n",
      "(19515, 200)\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CNN model.\n"
     ]
    }
   ],
   "source": [
    "print('Training CNN model.')\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(2, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM model.\n"
     ]
    }
   ],
   "source": [
    "print('Training LSTM model.')\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Bidirectional(LSTM(128, return_sequences=True))(embedded_sequences)\n",
    "x = Bidirectional(LSTM(128))(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(2, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Mutli-mask CNN model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=64, kernel_size=2)`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=64, kernel_size=3)`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=64, kernel_size=4)`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "print('Training Mutli-mask CNN model.')\n",
    "\n",
    "convs = []\n",
    "filter_sizes = [2, 3, 4]\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "# embedded_sequences = Dropout(rate=0.3)(embedded_sequences)\n",
    "\n",
    "for fsz in filter_sizes:\n",
    "    l_conv = Conv1D(nb_filter=64, filter_length=fsz, activation='relu')(embedded_sequences)\n",
    "    l_pool = MaxPooling1D(5)(l_conv)\n",
    "    convs.append(l_pool)\n",
    "\n",
    "l_merge = Concatenate(axis=1)(convs)\n",
    "l_flatten = Flatten()(l_merge)\n",
    "# l_flatten = Dropout(rate=0.3)(l_flatten)\n",
    "l_dense = Dense(64, activation='relu')(l_flatten)\n",
    "preds = Dense(2, activation='softmax')(l_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_58 (InputLayer)        (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_11 (Embedding)     (None, 200, 200)          3903000   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 200, 256)          336896    \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 4,667,290\n",
      "Trainable params: 764,290\n",
      "Non-trainable params: 3,903,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "earlystopping = EarlyStopping(patience=2)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_56 (InputLayer)        (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_11 (Embedding)     (None, 200, 200)          3903000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 256)          336896    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 4,667,290\n",
      "Trainable params: 764,290\n",
      "Non-trainable params: 3,903,000\n",
      "_________________________________________________________________\n",
      "Train on 10592 samples, validate on 2648 samples\n",
      "Epoch 1/10\n",
      "10592/10592 [==============================] - 257s 24ms/step - loss: 0.5728 - acc: 0.7088 - val_loss: 0.4975 - val_acc: 0.7685\n",
      "Epoch 2/10\n",
      "10592/10592 [==============================] - 223s 21ms/step - loss: 0.4932 - acc: 0.7599 - val_loss: 0.4688 - val_acc: 0.7700\n",
      "Epoch 3/10\n",
      "10592/10592 [==============================] - 249s 23ms/step - loss: 0.4623 - acc: 0.7825 - val_loss: 0.4545 - val_acc: 0.7859\n",
      "Epoch 4/10\n",
      "10592/10592 [==============================] - 234s 22ms/step - loss: 0.4431 - acc: 0.7941 - val_loss: 0.4591 - val_acc: 0.7825\n",
      "Epoch 5/10\n",
      "10592/10592 [==============================] - 223s 21ms/step - loss: 0.4230 - acc: 0.8044 - val_loss: 0.4665 - val_acc: 0.7915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a11c2c18>"
      ]
     },
     "execution_count": 808,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_data=(x_val, y_val),\n",
    "          callbacks=[earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = read_csv('data/testset-levela.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15923</td>\n",
       "      <td>#WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27014</td>\n",
       "      <td>#ConstitutionDay is revered by Conservatives, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30530</td>\n",
       "      <td>#FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13876</td>\n",
       "      <td>#Watching #Boomer getting the news that she is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60133</td>\n",
       "      <td>#NoPasaran: Unity demo to oppose the far-right...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet\n",
       "0  15923  #WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...\n",
       "1  27014  #ConstitutionDay is revered by Conservatives, ...\n",
       "2  30530  #FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...\n",
       "3  13876  #Watching #Boomer getting the news that she is...\n",
       "4  60133  #NoPasaran: Unity demo to oppose the far-right..."
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(df_test['tweet'])\n",
    "x_test = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "860/860 [==============================] - 8s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gold = read_csv('data/labels-levela.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = list(map(lambda x: label_index[x], df_gold[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7612403100775194"
      ]
     },
     "execution_count": 812,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-task b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_b = []\n",
    "for l in labels['b']:\n",
    "    if isinstance(l, int):\n",
    "        labels_b.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 830,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(False, int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " False,\n",
       " 0,\n",
       " False,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 0,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 0,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 0,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 0,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " 0,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 0,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 0,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 0,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " 0,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 0,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 0,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " False,\n",
       " False,\n",
       " 0,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 0,\n",
       " False,\n",
       " 0,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 0,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " 0,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 0,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " False,\n",
       " 1,\n",
       " 0,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " 0,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 0,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 0,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 0,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " 1,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " ...]"
      ]
     },
     "execution_count": 827,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gold = read_csv('data/labels-levelb.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = list(map(lambda x: label_index[x], df_gold[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7612403100775194"
      ]
     },
     "execution_count": 812,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_true, y_pred, average='macro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
